# Arbitrary Graph-Attention Transformers

Transformer models with arbitrary graph attention patterns.

TODO list:
 * [ ] training code
   * [x] instantiate the model
   * [ ] jit + evaluate the model
   * [ ] training loop
 * [ ] add attention patterns
   * [x] fully connected attn pattern
   * [ ] longformer-style attn pattern
