# Arbitrary Graph-Attention Transformers

Transformer models with arbitrary graph attention patterns.

TODO list:
 * [ ] training code
   * [x] instantiate the model
   * [x] jit + evaluate the model
   * [x] training loop
   * [ ] training working with long inputs
 * [x] add attention patterns
   * [x] fully connected attn pattern
   * [x] window attn pattern
   * [x] longformer-style attn pattern
 * [x] unit tests 
